{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":121},"id":"f70-gaBmoJGm"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..99.68872].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [1.4648438..99.18213].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..98.80371].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.04272461..96.936035].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..99.975586].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.018310547..97.15576].\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.5493164..96.58203].\n"]}],"source":["# @title Image Colorization GAN (Complete Working Version)\n","# @markdown ### Setup\n","%%capture\n","!pip install torch torchvision opencv-python tqdm matplotlib --quiet\n","!nvidia-smi  # Verify GPU\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import os\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# @title Configuration\n","# @title Configuration (Updated Single Version)\n","class Config:\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    num_epochs = 30\n","    batch_size = 16\n","    image_size = 256  # Must be 256 to match discriminator expectations\n","    dataset_path = \"/content/drive/MyDrive/MINIPROJECT/train2017\"\n","    save_dir = \"/content/drive/MyDrive/MINIPROJECT/colorization_results/saved_images\"\n","    checkpoint_dir = \"/content/drive/MyDrive/MINIPROJECT/colorization_results/checkpoints\"\n","\n","    @classmethod\n","    def initialize_dirs(cls):\n","        os.makedirs(cls.save_dir, exist_ok=True)\n","        os.makedirs(cls.checkpoint_dir, exist_ok=True)\n","\n","# Initialize directories\n","Config.initialize_dirs()\n","\n","# Mixed precision training\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# @title Dataset Class\n","class ColorizationDataset(Dataset):\n","    def __init__(self, root_dir):\n","        self.root_dir = Path(root_dir)\n","        valid_extensions = ('.png', '.jpg', '.jpeg', '.JPG', '.PNG')\n","        self.image_files = [f for f in self.root_dir.glob('*') if f.suffix.lower() in valid_extensions]\n","\n","        if not self.image_files:\n","            available = [f.suffix for f in self.root_dir.glob('*')]\n","            raise ValueError(f\"No valid images found. Found extensions: {set(available)}\")\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((Config.image_size, Config.image_size)),\n","            transforms.ToTensor()\n","        ])\n","\n","    def __len__(self): return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            with Image.open(str(self.image_files[idx])) as img:\n","                img = img.convert('RGB')\n","                img = self.transform(img)\n","                img = img.permute(1, 2, 0).numpy()\n","                lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","                L = torch.FloatTensor(lab_img[:,:,0]) / 50.0 - 1.0\n","                ab = torch.FloatTensor(lab_img[:,:,1:].transpose(2, 0, 1)) / 110.0\n","                return L.unsqueeze(0), ab\n","        except Exception as e:\n","            print(f\"Error loading {self.image_files[idx]}: {e}\")\n","            H, W = Config.image_size, Config.image_size\n","            return torch.rand(1, H, W)*2-1, torch.rand(2, H, W)\n","\n","# @title Model Architecture\n","class UNetDown(nn.Module):\n","    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n","        super().__init__()\n","        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n","        if normalize: layers.append(nn.InstanceNorm2d(out_size))\n","        layers.extend([nn.LeakyReLU(0.2), nn.Dropout(dropout)] if dropout else [nn.LeakyReLU(0.2)])\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x): return self.model(x)\n","\n","class UNetUp(nn.Module):\n","    def __init__(self, in_size, out_size, dropout=0.0):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n","            nn.InstanceNorm2d(out_size),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(dropout) if dropout else nn.Identity()\n","        )\n","\n","    def forward(self, x, skip_input):\n","        x = self.model(x)\n","        return torch.cat((x, skip_input), 1)\n","\n","class GeneratorUNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.down1 = UNetDown(1, 64, normalize=False)\n","        self.down2 = UNetDown(64, 128)\n","        self.down3 = UNetDown(128, 256)\n","        self.down4 = UNetDown(256, 512, dropout=0.5)\n","        self.down5 = UNetDown(512, 512, dropout=0.5)\n","        self.down6 = UNetDown(512, 512, dropout=0.5)\n","        self.down7 = UNetDown(512, 512, dropout=0.5)\n","        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n","\n","        self.up1 = UNetUp(512, 512, dropout=0.5)\n","        self.up2 = UNetUp(1024, 512, dropout=0.5)\n","        self.up3 = UNetUp(1024, 512, dropout=0.5)\n","        self.up4 = UNetUp(1024, 512, dropout=0.5)\n","        self.up5 = UNetUp(1024, 256)\n","        self.up6 = UNetUp(512, 128)\n","        self.up7 = UNetUp(256, 64)\n","\n","        self.final = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.ZeroPad2d((1, 0, 1, 0)),\n","            nn.Conv2d(128, 2, 4, padding=1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        d1 = self.down1(x)\n","        d2 = self.down2(d1)\n","        d3 = self.down3(d2)\n","        d4 = self.down4(d3)\n","        d5 = self.down5(d4)\n","        d6 = self.down6(d5)\n","        d7 = self.down7(d6)\n","        d8 = self.down8(d7)\n","        u1 = self.up1(d8, d7)\n","        u2 = self.up2(u1, d6)\n","        u3 = self.up3(u2, d5)\n","        u4 = self.up4(u3, d4)\n","        u5 = self.up5(u4, d3)\n","        u6 = self.up6(u5, d2)\n","        u7 = self.up7(u6, d1)\n","        return self.final(u7)\n","\n","# @title Fixed Discriminator Architecture\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        def block(in_filters, out_filters, normalization=True):\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n","            if normalization: layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(3, 64, normalization=False),\n","            *block(64, 128),\n","            *block(128, 256),\n","            *block(256, 512),\n","            nn.ZeroPad2d((1, 1, 1, 1)),  # Adjusted padding\n","            nn.Conv2d(512, 1, 4, padding=1, bias=False)  # Outputs 30x30\n","        )\n","\n","    def forward(self, img_A, img_B):\n","        return self.model(torch.cat((img_A, img_B), 1))\n","# Must be 256 to match discriminator expectations\n","    # ... (keep other config parameters the same)\n","\n","# @title Training Setup\n","def initialize_training():\n","    generator = GeneratorUNet().to(Config.device)\n","    discriminator = Discriminator().to(Config.device)\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    criterion_GAN = nn.MSELoss()\n","    criterion_L1 = nn.L1Loss()\n","\n","    # Verify dataset path\n","    if not Path(Config.dataset_path).exists():\n","        raise FileNotFoundError(f\"Path not found: {Config.dataset_path}\")\n","\n","    dataset = ColorizationDataset(Config.dataset_path)\n","    dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    return generator, discriminator, optimizer_G, optimizer_D, criterion_GAN, criterion_L1, dataloader\n","\n","# @title Training Loop\n","def train_model():\n","    generator, discriminator, optimizer_G, optimizer_D, criterion_GAN, criterion_L1, dataloader = initialize_training()\n","\n","    for epoch in range(Config.num_epochs):\n","        generator.train()\n","        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{Config.num_epochs}')\n","\n","        for i, (real_A, real_B) in enumerate(progress_bar):\n","            real_A, real_B = real_A.to(Config.device), real_B.to(Config.device)\n","\n","            # === Dynamically determine output size of the discriminator ===\n","            with torch.no_grad():\n","                output_shape = discriminator(real_A, real_B).shape[2:]\n","            valid = torch.ones((real_A.size(0), 1, *output_shape), device=Config.device)\n","            fake = torch.zeros_like(valid)\n","\n","            # ======== Train Discriminator ========\n","            optimizer_D.zero_grad()\n","            with torch.cuda.amp.autocast():\n","                fake_B = generator(real_A)\n","                pred_real = discriminator(real_A, real_B)\n","                loss_real = criterion_GAN(pred_real, valid)\n","                pred_fake = discriminator(real_A, fake_B.detach())\n","                loss_fake = criterion_GAN(pred_fake, fake)\n","                loss_D = (loss_real + loss_fake) * 0.5\n","            scaler.scale(loss_D).backward()\n","            scaler.step(optimizer_D)\n","\n","            # ======== Train Generator ========\n","            optimizer_G.zero_grad()\n","            with torch.cuda.amp.autocast():\n","                pred_fake = discriminator(real_A, fake_B)\n","                loss_GAN = criterion_GAN(pred_fake, valid)\n","                loss_L1 = criterion_L1(fake_B, real_B) * 100\n","                loss_G = loss_GAN + loss_L1\n","            scaler.scale(loss_G).backward()\n","            scaler.step(optimizer_G)\n","            scaler.update()\n","\n","            # Update progress bar\n","            progress_bar.set_postfix({\n","                'D_loss': loss_D.item(),\n","                'G_loss': loss_G.item(),\n","                'L1': loss_L1.item()\n","            })\n","\n","        # ======== Save model checkpoints and sample images ========\n","        torch.save(generator.state_dict(), f\"{Config.checkpoint_dir}/generator_{epoch}.pth\")\n","\n","        if epoch % 5 == 0 or epoch == Config.num_epochs - 1:\n","            with torch.no_grad():\n","                fake_B = generator(real_A[:1])\n","                fake_B = fake_B.cpu().numpy().transpose(0, 2, 3, 1)[0] * 110.0\n","                real_A_ = (real_A[0].cpu().numpy().squeeze() + 1) * 50.0\n","                colorized = cv2.cvtColor(np.concatenate([real_A_[..., np.newaxis], fake_B], axis=2).astype(np.uint8), cv2.COLOR_LAB2RGB)\n","\n","                plt.figure(figsize=(10, 5))\n","                plt.subplot(1, 2, 1)\n","                plt.imshow(real_A_[..., np.newaxis].repeat(3, axis=2), cmap='gray')\n","                plt.title(\"Input (L)\")\n","\n","                plt.subplot(1, 2, 2)\n","                plt.imshow(colorized)\n","                plt.title(f\"Colorized (Epoch {epoch})\")\n","                plt.savefig(f\"{Config.save_dir}/epoch_{epoch}.png\")\n","                plt.show()\n","\n","\n","\n","\n","# @title Start Training\n","print(\"=== System Check ===\")\n","print(f\"Device: {Config.device}\")\n","print(f\"Dataset path: {Config.dataset_path}\")\n","print(f\"Image size: {Config.image_size}\")\n","print(f\"Batch size: {Config.batch_size}\")\n","\n","train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38360,"status":"ok","timestamp":1745356941735,"user":{"displayName":"P24cs006_Devnarayan_ Bhavsar","userId":"08080645020955087281"},"user_tz":-330},"id":"aXXgU8xzYE0M","outputId":"0073d0f0-24a5-4701-93e7-5843c1a3efa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["000000061409.jpg\n","000000061439.jpg\n","000000061463.jpg\n","000000061492.jpg\n","000000061503.jpg\n"]}],"source":["!ls \"/content/drive/MyDrive/MINIPROJECT/train2017\" | head -n 5  # Show first 5 files"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPeCB2urvC6Tsqhe1bGmrrO","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}